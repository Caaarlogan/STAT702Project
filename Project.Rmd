---
title: "Project"
author: "Carlo Carbonilla"
date: "13/05/2021"
output:
  html_document:
    df_print: paged
---

Load the required Packages.

```{r}
#Loading the Packages
library(dplyr)
library(tidyverse)
library(lubridate)
library(wordcloud)
library('reshape2')
library(moments)
library(nortest)
library("MASS")
```

Question 1. 

Solution 1.a) 

Import the data from STAT702_project_sales_data.csv file.

```{r}
sales <- read.csv(file = 'STAT702_project_sales_data.csv')
head(sales)
```

To format week as a date, first step is to Convert week column from character to date type.
And then, use the lubridate function to extract the month.

```{r}
sales$week <- as.Date(sales$week, format =  "%d/%m/%y")
head(sales)
```

Given the project requirements, Filtering dataframe to dates in 2012 and with the product sku of 216419 (assigned to group #2)

```{r}
productSales <- sales %>% filter(sku_id == 216419)
productSales
```

Here the variables Month and Year witholds the total number of units sold by month and year respectively,

```{r}
Month <- month(productSales$week)
Year <- year(productSales$week)

monthlySales <- aggregate(cbind(units_sold)~Month+Year,
             data=productSales,FUN=sum)

monthlySales$MonthYear <- as.Date(paste(1,monthlySales$Month,monthlySales$Year,sep="/"), format="%d/%m/%Y")

monthlySales
```

There are 31 rows resides in the table where each row represents a month starting from January 2011 all the way till July 2013 (31 Months).
Furtherfore, this table has laid the foundation to help plotting the data on a graph to observe any trends/patterns. 

```{r}
ggplot(monthlySales, aes(MonthYear, units_sold, group=1)) + 
  geom_line() +
  geom_point() +
  scale_x_date(date_breaks = "1 month", date_labels = "%m-%Y") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ggtitle("Monthly Sales") +
  xlab("Month and Year") +
  ylab("Units Sold")
```


This plot shows that the montly sales over a period of time for the Product ID- 216419 which has been fluctuating with in \$15000 and \$25000. However, the sales went unreasonably over $35000 in January 2012 which could be either because of New year's holiday weekend or some other unknown factors. It's best to remove the July 2013 sales out of the analysis it only contains 7 days sales total. 
To sum it up, this plot describes the increase and decrease in product sale each month



##Solution 1.b) 

First of all, we need to sum the total units sold by each store, so that further analysis can be drawn.

```{r}
StoreID <- productSales$store_id

storeSales <- aggregate(cbind(units_sold)~StoreID,
             data=productSales,FUN=sum)

storeSales$StoreID <- as.character(storeSales$StoreID)

storeSales
```

Store Id with respect to units sold by that store. Store with ID:8023 has max number of sales which is close to 29000 units followed by store ID 9613.
In order to draw comparison, let's make a plot using geom_bar and see the sales trend between the various stores. However, there are some stores where the sale numbers are relatively low. For instance, store id: 9001 sold the least number of units (1095 only). Therefore, apprximately 9000 is the average unit sold ratio among all the store. 

```{r}
ggplot(data=storeSales, aes(x=reorder(StoreID, -units_sold), y=units_sold)) +
  geom_bar(stat="identity", fill="steelblue") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(title = "Total Units Sold of Stores", y = "Units Sold", x = "Store ID")
```

Summary of sales of all the store collectively. Looking at the graph above we get an idea of what a good performing store looks like and on the right hand side of the graph are the stores that are not performing so well. 

```{r}
summary(storeSales)
```

Given there are 67 stores in total with the sales mean of ~9000 units per store. Therefore, the stores that had sold more that 9000 units can be considered as good performing store. On the other hand, stores that sold below 9000 units can be categorized as the less/poor performing stores.
To drill this down further, making a new variable called top10Sales which withholds the top 10 performing stores based of the number of units sold.

```{r}
top10Sales <- head(storeSales %>% arrange(desc(units_sold)),10)
top10Sales
```

All of those 10 store are well above the mean calculated earlier.

Graph of Top 10 Stores

```{r}
ggplot(data=top10Sales, aes(x=reorder(StoreID, -units_sold), y=units_sold)) +
  geom_bar(stat="identity", fill="steelblue") +
  geom_text(aes(label=units_sold), vjust=1.6, color="white", size=3.5) +
  labs(title = "Top 10 Stores in Units Sold", y = "Units Sold", x = "Store ID")
```
```{r}
summary(top10Sales)
```

Hence, this is the data that will be presented to the General Manager Sales.

Similarly, bottom10Sales variable withholds the sales record of the 10 least performing stores. 

```{r}
bottom10Sales <- head(storeSales %>% arrange(units_sold),10)
bottom10Sales
```

Graph of Bottom 10 Stores.

```{r}
ggplot(data=bottom10Sales, aes(x= reorder(StoreID, units_sold), y=units_sold)) +
  geom_bar(stat="identity", fill="steelblue") +
  geom_text(aes(label=units_sold), vjust=1.6, color="white", size=3.5) +
  labs(title = "Bottom 10 Stores in Units Sold", y = "Units Sold", x = "Store ID")
```
```{r}
summary(bottom10Sales)
```

All of the store above are well below the mean calculated earlier. Therefore, this step by step approach is useful to understand the sale trends between the stores and how well/poor are they performing. There are store with high selling numbers which could be based on the customer base, location of the store of the selling techniques used by employees (Sales driven culture) where as there are store with less selling numbers which again could depend upon the various factors. 
However, the data and the visuals created above put a baseline for the Management to create strategies on how the sale can be lifted and what are the stores that requires  more attention.



Question 2.

(a)

i. Businesses these days have adapted the concept of overstocking for the purpose of satisfying customer needs in a specific period of time. But it turns out to be a costly process and requires high capital investment. Therefore, ir is very vital for companies to do an adequate stock control and handle inventory on issues in realtime. Hence, the Economic Order Quantity model (EOQ model) comes into existance. 

To perform the required analysis, first step is to filter the dataframe to include only the dates with 2012 with the product SkuID = 216233

```{r}
productSales2012 <- sales %>% filter(week >= as.Date("2012-01-01"), week <= as.Date("2012-12-31"), sku_id == 216233)
head(productSales2012)
```

k = setup cost
A = number of items demanded per annum
c = purchase cost per item
h = holding cost per item per annum
Q* = optimum order quantity (number of items)
T = time between order = length of the inventory cycle = Q/A

Step 2 is to calculate the total units sold by all the stores collectively over the year 2012.

```{r}
A <- sum(productSales2012$units_sold) # Annual demand
A
```

Here, we got the total number of units sold by the store are 169591

Thus, the calculations performed aboves gives us the base to calculate the optimum order quantity (Q)

```{r}
k <- 130 # ordering cost
h <- 1.5 # holding cost
Q <- sqrt((2*k*A) / h) # optimum order quantity 
round(Q,0)
```
optimum order quantity = 5422

Find time between the orders
```{r}
T <- 365 * sqrt(2*k/(A*h)) # inventory cycle 
round(T,0)
```
Time between the orders T aka Inventory cycle = 12

Find annual inventory cost
```{r}
inv_cost_ann <- k*(A/Q) + h*(Q/2) # annual inventory cost
inv_cost_ann
```

To summarize the analysis above, there were 169591 units of this product was sold in 2012 and is assumed to be A, the number of items demanded per annum. Based on the 2012 sales, the company should order 5422 units of the product every 12 days. The annual inventory cost is $8132.68


ii.

S = maximum inventory level after outstanding backorders have been met

Find backorder cost
```{r}
p <- 0.05 * (k+h)
p
```
Find economic order quantity
```{r}
Q <- sqrt(2*k*A/h) * sqrt((p+h)/p)
Q <- round(Q, 0)
Q
```

Find maximum level stock
```{r}
S <- sqrt(2*k*A/h) * sqrt(p/(p+h))
S <- round(S, 0)
S
```

Find optimum time between orders
```{r}
t <- 365*Q/A
t <- round(t,0)
t
```

Find in stock times
```{r}
t1 <- 365*(S/A)
t1 <- round(t1,0)
t1
```

Find out of stock times
```{r}
t2 <- t-t1
t2
```

Plot back order model
```{r}
plot(0, xlim = c(0, 2*t), ylim = c(-Q+S, S), type = "n",
     xlab = "Time (days)", ylab = "Stock level",
     main = "Inventory cycles for product 216233")

segments(x0 = c(0,t), y0 = S, x1 = c(t,2*t), y1 = -Q+S) # diagonals
segments(x0 = c(0,t), y0 = -Q+S, x1 = c(0,t), y1 = S)   # vertical
segments(x0 = 0, y0 = 0, x1 = 2*t, y1 = 0)              # horizontal
```


Find proportion of time the company have to take backorders
```{r}
round(100*(t-t1)/t,0)
```


Find total annual inventory cost
```{r}
round(k*A/Q + h/2 * S^2/Q + p/2 * (Q-S)^2 /Q,2)
```


(b)

i.

```{r}
productSales2012 <- sales %>% filter(week >= as.Date("2012-01-01"), week <= as.Date("2012-12-31"), sku_id == 216425)
head(productSales2012)
```


Find weekly units sold
```{r}
saleWeek <- productSales2012$week

weeklySales <- aggregate(cbind(units_sold)~saleWeek,
             data=productSales2012,FUN=sum)

weeklySales
```


Graph weekly units sold
```{r}
hist(weeklySales$units_sold, main = "Demand for product 216425", 
xlab = "Weekly units sold")
```

```{r}
mean(weeklySales$units_sold)
sd(weeklySales$units_sold)
```

```{r}
skewness(weeklySales$units_sold)
kurtosis(weeklySales$units_sold)
```

Based on the histogram, the distribution of demand seems to be normal. However, the skewness of the data is about -0.48 which is not very close 
to 0. The negative value indicates that the data that are skewed left. In addition, the kurtosis is about 3.91 which is greater than 3, which tends to have heavier tails than the number distribution. So, before fitting, we need to conduct goodness-of-fit test.


```{r}
shapiro.test(weeklySales$units_sold)
```

```{r}
lillie.test(weeklySales$units_sold)
```

The p-value for both tests is higher 0.05, we accept null hypothesis that data come from a normal distribution.

Fitting normal distribution
```{r}
fitnorm <- fitdistr(weeklySales$units_sold, "normal")
fitnorm
```

Find reorder point
```{r}
qnorm(0.95, 2057, 518)
```


```{r}
hist(weeklySales$units_sold, main = "Demand for product 216425", xlab = "Lead-Time Demand", prob = TRUE)

curve(dnorm(x, 2057, 518), 
from=0, to=3500,
col="blue", lty=1, add=TRUE)

segments(x0 = 2909, y0 = 0, x1 = 2909, y1 = 8e-4)   # vertical
```

This is the lead time demand distribution, a normal distribution with a mean of 2057 and a standard deviation of 518. The reorder point r of 2909 allows 5% chance of a stock-out.


Find annual demand
```{r}
D <- 2057 * 52
D
```

The annual demand is the expected weekly demand multiplied by 52, the amount of weeks in a year, which is 106964


Find optimal order quantity
```{r}
co <- 20.5 # order cost
ch <- 6.5  # holding cost
Q <- round(sqrt(2 * D * co / ch), 0) # optimal order quantity
Q
```
The optimal order quantity is 821 units 

ii.

```{r}
productSales2012 <- sales %>% filter(week >= as.Date("2012-01-01"), week <= as.Date("2012-12-31"), sku_id == 216419)
head(productSales2012)
```


Find weekly units sold
```{r}
saleWeek <- productSales2012$week

weeklySales <- aggregate(cbind(units_sold)~saleWeek,
             data=productSales2012,FUN=sum)

weeklySales
```


Graph weekly units sold
```{r}
hist(weeklySales$units_sold, main = "Demand for product 216419", 
xlab = "Weekly units sold")
```

```{r}
mean(weeklySales$units_sold)
sd(weeklySales$units_sold)
```

```{r}
skewness(weeklySales$units_sold)
kurtosis(weeklySales$units_sold)
```

Based on the histogram, the distribution of demand seems to be normal. However, the skewness of the data is about 2.18 which is not very close 
to 0. The positive value indicates that the data that are skewed right. In addition, the kurtosis is about 9.64 which is greater than 3, which tends to have heavier tails than the number distribution. So, before fitting, we need to conduct goodness-of-fit test.


```{r}
shapiro.test(weeklySales$units_sold)
```

```{r}
lillie.test(weeklySales$units_sold)
```

The p-value for both tests is lower 0.05, we refuse the null hypothesis that data come from a normal distribution.

Fitting normal distribution
```{r}
fitnorm <- fitdistr(weeklySales$units_sold, "normal")
fitnorm
```

```{r}
hist(weeklySales$units_sold, main = "Demand for product 216425", xlab = "Weekly units sold", prob = TRUE)

curve(dnorm(x, fitnorm$estimate[1], fitnorm$estimate[2]), 
from=0, to=max(weeklySales$units_sold),
col="blue", lty=1, add=TRUE)
```

This is the lead time demand distribution, a normal distribution with a mean of 4582 and a standard deviation of 1260.


Find annual demand
```{r}
D <- 4582 * 52
D
```

The annual demand is the expected weekly demand multiplied by 52, the amount of weeks in a year, which is 106964


Find optimal order quantity
```{r}
co <- 20.5 # order cost
ch <- 6.5  # holding cost
Q <- round(sqrt(2 * D * co / ch), 0) # optimal order quantity
Q
```


Question 3.

Load the review data file from the repository. 

```{r}
review <- read.csv(file = 'STAT702_project_reviews_data.csv')
review <- as_tibble(review)
head(review)
```

Filter the product B00005249G (asin) assigned to our group

```{r}
review <- review
selectedProduct <- review %>% filter(asin == "B00005249G")
selectedProduct
```

```{r}
summary(selectedProduct)
```

ggplot the overall review ratings for the product.

```{r}
ggplot(selectedProduct, aes(overall)) + 
  geom_bar() +
  ggtitle("Overall Review Rating for Product ID: B00005249G")
```


Looking the plot above, majority of the customers rated 5 follwed by a few 4 then 3, 2 and down to 1. The dataset contained total 1,430 rows and each row withholds a customer rating on a scale of 1 to 5. 5 being the product is of good quality (use) and 1 being not satistied with the product experience. over 1000 customers selected 5 followed by over 160 selected 4 and so on. The mean review ratings is 4.669


 

3b) As per requirements, this task requires analyzing the reviewText column and provide some trends to the GM.

Reading the selectedProduct in data variable and creating a column to record each row number.


```{r}
data <- selectedProduct %>%
mutate(linenumber = row_number());
head(data)
```

Install and load tidytext to clean and tidy text data

```{r}
install.packages('tidytext')
```


```{r}
library(tidytext)
```


Tokenization: “A token is a meaningful unit of text, most often a word, that we are interested in using for further analysis, and tokenization is the process of splitting text into tokens.” [Silge and Robinson, ]

```{r}
selectedProduct %>%
unnest_tokens(output = word, input = reviewText) -> tokenised_data
```

Removing stopwords.
Stop words are the “little” words like “the”, “of”, “to” etc

```{r}
tokenised_data %>%
  group_by(document.id) %>% 
  ungroup() %>% 
  anti_join(stop_words) -> cleanData
  
```

Plotting the cleanData after it's tidy enough for analysis.

```{r}
cleanData %>% count(word, sort = TRUE) %>% filter(n > 100) %>% mutate(word =  reorder(word, n)) %>% 
ggplot(aes (word, n)) +
  geom_col() +
  ggtitle("All the words with the used frequency over 100")

  
```

Looking at the plot above, there are the most common used words in the reviewText section. "Pens" being mentioned the most for over 900 times where as "pen" being the second most counts about 400 times. On the other hand, words like "box", "cheap", "wiriting" were used just over 100 times.


Word cloud of frequently used words.

```{r}
#word Cloud

cleanData %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
  
```


Sentiment Analysis - Allocate sentiment to each word using bing. Sentimental analysis helps to shows if the users/customers are happy with the product experience or not. Here, we categorized the sentiments as a set of negative and positive words.


```{r}

# Sentiment allocation to each word
cleanData %>% 
  inner_join(get_sentiments("bing"),  by = c("word"  = "word")) %>%
  count(word, sentiment, overall, sort = TRUE)  -> sentiment_data

# Plot positive/negative words
sentiment_data %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment", 
       x = NULL) + 
  coord_flip()


```

The above graph shows the sentiment analysis contribution of the Negative words on left hand side and the positive words on the right hand side. Here, we can see that cheap is the word classified as negative which in reality could be a positive expression like a product is cleap compare to other markets. Other ones are like, hard, lose, wrong, expensive, losing, leak, bad, lost, complaints. This gives indication to the GM on what are focus areas.
On the other hand, love is the positive word being used the most over 100 times followed nice, smooth, fine, smoothly, perfect, favourite happy etc. 


```{r}
sentiment_data %>% 
  ggplot() +
  geom_bar(aes(x = overall, fill = sentiment), position = "fill") +
  ggtitle("Positive and negative sentiment ratio") +
  xlab("Overall Rating") +
  ylab("Sentiment ratio")

```

From the alove analyis, we can see that the sentiment analysis works well for the negative words but not so well for the positive works, 

Therefore, to get an overview of the words, we will create a sentiment word cloud.


Word cloud using the negative and positive words.

```{r}
cleanData %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("darkred", "darkgreen"),
title.colors=c("darkred", "darkgreen"),
max.words = 100, title.size = 2)

```

The words cloud shows the most used keywords within in the reviewText section. It correspond to the  an overview of the words that were used in the reviewText and it saves GM time to manually go through each remove comment. It shows if the customers are happy and satisfied with the product or not. And also give a general trend of why customer's are liking the product vs what are the areas to be considered for future decision strategy plannings. A lot of companies use "NPS" known as Net promoter score to gaage their customer satisfaction. This should give a wider view of the reviews to the management.
In conclusion, we can say that most of the customers are loving the product and gave out good ratings.





